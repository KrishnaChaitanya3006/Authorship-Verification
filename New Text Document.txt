# Install dependencies
!pip install torch torch-geometric transformers nltk

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import TransformerConv
from torch_geometric.data import Data
from transformers import BertTokenizer, BertModel
from nltk import word_tokenize, sent_tokenize
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, f1_score
import nltk

# Download NLTK resources
nltk.download('punkt')
nltk.download('wordnet')

# Function to generate co-occurrence graph
def create_cooccurrence_graph(text, window_size=2):
    words = word_tokenize(text.lower())
    vocab = list(set(words))
    word_to_idx = {word: i for i, word in enumerate(vocab)}
    
    # Create edges based on co-occurrence
    edges = []
    for i in range(len(words)):
        for j in range(i + 1, min(i + window_size + 1, len(words))):
            if words[i] != words[j]:
                edges.append([word_to_idx[words[i]], word_to_idx[words[j]]])
                edges.append([word_to_idx[words[j]], word_to_idx[words[i]]])
    
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous() if edges else torch.tensor([[], []], dtype=torch.long)
    
    # Node features: Random for simplicity (replace with BERT embeddings for better performance)
    x = torch.randn(len(vocab), 768)  # 768-dim to mimic BERT embeddings
    return Data(x=x, edge_index=edge_index)

# Stylometric feature extraction
def extract_stylometric_features(text):
    words = word_tokenize(text)
    sentences = sent_tokenize(text)
    # Mean word length
    mean_word_length = np.mean([len(word) for word in words]) if words else 0
    # Sentence length variability
    sent_lengths = [len(word_tokenize(sent)) for sent in sentences]
    sent_length_var = np.std(sent_lengths) if sent_lengths else 0
    # Punctuation frequency
    punctuation_count = sum(text.count(p) for p in '.,!?;:"\'')
    # Connector word usage
    connector_words = ['however', 'therefore', 'moreover', 'thus']
    connector_freq = sum(text.lower().count(word) for word in connector_words)
    return torch.tensor([mean_word_length, sent_length_var, punctuation_count, connector_freq], dtype=torch.float)

# GNN Model
class GNNModel(nn.Module):
    def _init_(self, input_dim, hidden_dim):
        super(GNNModel, self)._init_()
        self.conv1 = TransformerConv(input_dim, hidden_dim, heads=4, dropout=0.1)
        self.conv2 = TransformerConv(hidden_dim * 4, hidden_dim, heads=1, dropout=0.1)
        self.conv3 = TransformerConv(hidden_dim, hidden_dim, heads=1, dropout=0.1)
    
    def forward(self, x, edge_index):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        x = self.conv3(x, edge_index)
        return torch.mean(x, dim=0)  # Global mean pooling

# Dual-Path Model
class DualPathModel(nn.Module):
    def _init_(self, bert_model_name='bert-base-uncased', hidden_dim=256):
        super(DualPathModel, self)._init_()
        self.bert = BertModel.from_pretrained(bert_model_name)
        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)
        self.gnn = GNNModel(input_dim=768, hidden_dim=hidden_dim)
        self.stylo_fc = nn.Linear(4, hidden_dim)  # 4 stylometric features
        self.classifier = nn.Sequential(
            nn.Linear(hidden_dim + 768 + hidden_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(512, 2)  # Binary classification: human vs. machine
        )
    
    def forward(self, text, graph_data):
        # LLM Path: BERT embeddings
        inputs = self.tokenizer(text, return_tensors='pt', max_length=128, truncation=True, padding=True).to(graph_data.x.device)
        bert_out = self.bert(**inputs).last_hidden_state[:, 0, :]  # CLS token
        
        # GNN Path: Graph embeddings
        gnn_out = self.gnn(graph_data.x, graph_data.edge_index)
        
        # Stylometric features
        stylo_features = extract_stylometric_features(text)
        stylo_out = F.relu(self.stylo_fc(stylo_features.to(graph_data.x.device)))
        
        # Concatenate all features
        combined = torch.cat([bert_out.squeeze(0), gnn_out, stylo_out], dim=0)
        
        # Final classification
        return self.classifier(combined)

# Training function
def train_model(model, train_data, val_data, epochs=5, device='cuda'):
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
    criterion = nn.CrossEntropyLoss()
    
    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for text, graph_data, label in train_data:
            graph_data = graph_data.to(device)
            label = torch.tensor([label], dtype=torch.long).to(device)
            optimizer.zero_grad()
            out = model(text, graph_data)
            loss = criterion(out.unsqueeze(0), label)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        
        # Validation
        model.eval()
        val_preds, val_labels = [], []
        with torch.no_grad():
            for text, graph_data, label in val_data:
                graph_data = graph_data.to(device)
                out = model(text, graph_data)
                val_preds.append(F.softmax(out, dim=0)[1].item())
                val_labels.append(label)
        
        roc_auc = roc_auc_score(val_labels, val_preds)
        f1 = f1_score([1 if p > 0.5 else 0 for p in val_preds], val_labels)
        print(f'Epoch {epoch+1}, Loss: {total_loss / len(train_data):.4f}, Val ROC-AUC: {roc_auc:.4f}, Val F1: {f1:.4f}')

# Load and preprocess synthetic data
def load_and_preprocess_data(csv_path):
    df = pd.read_csv(csv_path)
    data = []
    for _, row in df.iterrows():
        text = row['text']
        label = row['label']
        graph_data = create_cooccurrence_graph(text)
        data.append((text, graph_data, label))
    return data

# Main execution
if _name_ == "_main_":
    # Check for GPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'Using device: {device}')
    
    # Load synthetic data from CSV
    synthetic_data_csv = """
text,label
"The sun sets slowly behind the mountain, casting a warm glow over the calm valley.",0
"The sun descends gradually behind the peak, emitting a warm light over the serene valley.",1
"I walked through the forest, feeling the crunch of leaves under my feet!",0
"I traversed the woodland, sensing the crackle of foliage beneath my steps.",1
"She smiled brightly, her eyes reflecting the joy of the moment...",0
"She grinned widely, her eyes mirroring the happiness of the occasion.",1
"The river flows gently, its waters sparkling under the midday sun.",0
"The river runs smoothly, its surface gleaming under the noon sunlight.",1
"He sat quietly, pondering the meaning of life with a heavy heart.",0
"He rested silently, contemplating life's purpose with a solemn mood.",1
"The city buzzed with energy, lights flashing in the vibrant night.",0
"The metropolis hummed with vitality, lights flickering in the lively evening.",1
"The breeze whispered softly, carrying the scent of blooming flowers.",0
"The wind murmured gently, bearing the fragrance of blossoming blooms.",1
"They laughed together, sharing stories by the crackling campfire.",0
"They chuckled collectively, exchanging tales by the glowing fire.",1
"The stars twinkled above, painting the sky with dreams of tomorrow.",0
"The stars sparkled overhead, adorning the sky with visions of the future.",1
"The old house creaked, its walls telling tales of forgotten years.",0
"The aged dwelling groaned, its structure narrating stories of past decades.",1
"""
    with open('synthetic_data.csv', 'w') as f:
        f.write(synthetic_data_csv)
    
    # Load and split data
    data = load_and_preprocess_data('synthetic_data.csv')
    train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)
    
    # Initialize and train model
    model = DualPathModel()
    train_model(model, train_data, val_data, epochs=5, device=device)